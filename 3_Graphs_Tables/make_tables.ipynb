{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55e0c3e",
   "metadata": {},
   "source": [
    "# make_tables.ipynb\n",
    "**Purpose:** Generate LaTeX tables used in the manuscript (e.g., tuned hyperparameters, feature rankings, thresholds).  \n",
    "**Inputs:** Excel/CSV outputs from Stage 1 and Stage 2 tuning scripts.  \n",
    "**Outputs:** LaTeX `.txt` table files.\n",
    "\n",
    "**Part of repository:** `3_Graphs_Tables`  \n",
    "**Reproducibility:** This notebook is deterministic given the provided model output files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f251f104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cara/anaconda3/envs/2stagemodel/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from make_outputs import format_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd7c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDIT_LABELS = {\n",
    "    \"conspiracy\": \"r/Conspiracy\",\n",
    "    \"crypto\": \"r/CryptoCurrency\",\n",
    "    \"politics\": \"r/politics\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2cffa09",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1136838261.py, line 14)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"footer\":\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# want header - title - table_header - tabular - table_footer - caption - footer\n",
    "\n",
    "supp_mat_table = {\n",
    "    \"header\": r\"\\paragraph*{S Table.}{\\bf \",\n",
    "    \"table_header\":\n",
    "r\"\"\"}\n",
    "\\label{S-Table.}\n",
    "\\begin{table}[!ht]\n",
    "\\centering\"\"\",\n",
    "    \"table_footer\":\n",
    "r\"\"\"\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \"footer\":\n",
    "r\"\"\"\n",
    "\\FloatBarrier\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "95b2fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_supp_mat_table(title, tabular, caption):\n",
    "    tabular = tabular.replace(r\"\\\\\", r\"\\\\ \\hline\")\n",
    "    to_repl = [r'\\toprule', r'\\midrule', r'\\bottomrule']\n",
    "    for s in to_repl:\n",
    "        tabular = tabular.replace(s, \"\")\n",
    "    return supp_mat_table[\"header\"] + title + supp_mat_table[\"table_header\"] + tabular + supp_mat_table[\"table_footer\"] + caption + supp_mat_table[\"footer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_dfs = {}\n",
    "sheet_name = \"all_hyperparams\"\n",
    "for subreddit in SUBREDDIT_LABELS:\n",
    "    eval_filepath = f\"/home/cara/Documents/reddit_analyses/thread-size/Outputs/1_thread_start/{subreddit}/4_model/evaluation.xlsx\"\n",
    "    df = pd.read_excel(eval_filepath, sheet_name=sheet_name)\n",
    "    \n",
    "    # Ensure correct column names\n",
    "    df.columns = [\"Features\", \"Parameter\", \"Value\"]\n",
    "\n",
    "    # Forward-fill N_feats to propagate block identifiers\n",
    "    df[\"Features\"] = df[\"Features\"].ffill().astype(int)\n",
    "\n",
    "    # Pivot into the desired wide format\n",
    "    table = df.pivot(index=\"Features\", columns=\"Parameter\", values=\"Value\")\n",
    "\n",
    "    hyperparam_dfs[subreddit] = table.copy()\n",
    "\n",
    "    table = table.reset_index()  # if N_feats was the index\n",
    "\n",
    "    latex_str = table.to_latex(\n",
    "        index=False,                    # don’t print the row index\n",
    "        header=True,\n",
    "        float_format=\"%.4f\",         # control float precision\n",
    "        column_format=\"|l|r|r|r|r|r|r|r|r|\",      # LaTeX alignment (1 left + 8 right)\n",
    "        #caption=f\"{SUBREDDIT_LABELS[subreddit]} tuned thread start LightGBM hyperparameters by number of features.\",\n",
    "        #label=f\"tab:s1-{subreddit}-hyperparams\",\n",
    "        escape=True                    # so underscores in col names are not escaped weirdly\n",
    "    )\n",
    "    \n",
    "    caption = f\"Optimal LightGBM tree hyperparameters selected via cross-validated Optuna/TPE search for each number of features for {SUBREDDIT_LABELS[subreddit]}. Values represent cross-fold aggregated hyperparameters, using the mode for integer parameters and the mean for continuous parameters. These configurations were used for the final thread start model evaluation.\"\n",
    "    title = f\"{subreddit} tuned thread start LightGBM hyperparameters by number of features.\"\n",
    "    full_latex = built_supp_mat_table(title, latex_str, caption)\n",
    "    # Save to file\n",
    "    with open(f\"/home/cara/Documents/reddit_analyses/thread-size/Publication_Outputs/1_Thread_Start/hyperparam_outputs/{subreddit}.txt\", \"w\") as f:\n",
    "        f.write(full_latex)\n",
    "\n",
    "outfile = f\"/home/cara/Documents/reddit_analyses/thread-size/Publication_Outputs/1_Thread_Start/hyperparam_outputs/hyperparams.xlsx\"\n",
    "with pd.ExcelWriter(outfile) as writer:\n",
    "    for sub, df in hyperparam_dfs.items():\n",
    "        df.to_excel(writer, sheet_name=sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb4dffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_threshold_dfs = {}\n",
    "sheet_name = \"all_params\"\n",
    "for subreddit in SUBREDDIT_LABELS:\n",
    "    eval_filepath = f\"/home/cara/Documents/reddit_analyses/thread-size/Outputs/1_thread_start/{subreddit}/4_model/evaluation.xlsx\"\n",
    "    df = pd.read_excel(eval_filepath, sheet_name=sheet_name)\n",
    "    \n",
    "    # Ensure correct column names\n",
    "    df.columns = [\"Features\", \"param_index\", \"Parameter\", \"Value\"]\n",
    "\n",
    "    # Forward-fill N_feats to propagate block identifiers\n",
    "    df[\"Features\"] = df[\"Features\"].ffill().astype(int)\n",
    "\n",
    "    # Pivot into the desired wide format\n",
    "    table = df.pivot(index=\"Features\", columns=\"Parameter\", values=\"Value\")\n",
    "\n",
    "    model_threshold_dfs[subreddit] = table.copy()[['model_threshold']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23bcd5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(model_threshold_dfs, axis=1).to_csv(f\"/home/cara/Documents/reddit_analyses/thread-size/Publication_Outputs/1_Thread_Start/metrics/model_thresholds.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b999a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outfile = f\"/home/cara/Documents/reddit_analyses/thread-size/Publication_Outputs/1_Thread_Start/hyperparam_outputs/hyperparams.xlsx\"\n",
    "with pd.ExcelWriter(outfile) as writer:\n",
    "    for sub, df in hyperparam_dfs.items():\n",
    "        df.to_excel(writer, sheet_name=sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ce70b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_cw_dfs = {}\n",
    "sheet_name = \"params\"\n",
    "for subreddit in SUBREDDIT_LABELS:\n",
    "    eval_filepath = f\"/home/cara/Documents/reddit_analyses/thread-size/Outputs/2_thread_size/{subreddit}/2_tuning/tuning_outputs.xlsx\"\n",
    "    df = pd.read_excel(eval_filepath, sheet_name=sheet_name)\n",
    "    \n",
    "    # Ensure correct column names\n",
    "    df = df[['n_feats', 'final_class_weights']].rename(columns={\n",
    "        \"final_class_weights\": \"cws\"\n",
    "    })\n",
    "\n",
    "    s2_cw_dfs[subreddit] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb370889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cw_string(s):\n",
    "    cleaned = re.sub(r\"np\\.float64\\(([^)]+)\\)\", r\"\\1\", s)\n",
    "    return ast.literal_eval(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "905bb0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\"Stalled\", \"Small\", \"Medium\", \"Large\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23c89be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"/home/cara/Documents/reddit_analyses/thread-size/Publication_Outputs/2_Thread_Size/tuning_outputs/class_weights.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(outfile) as writer:\n",
    "    for sub, df in s2_cw_dfs.items():\n",
    "        expanded = pd.json_normalize(df[\"cws\"].apply(parse_cw_string))\n",
    "        df = df.join(expanded)[[\"n_feats\", 0,1,2,3]]\n",
    "        ratios = df[['n_feats']].copy()\n",
    "        for col in [0,1,2,3]:\n",
    "            ratios[CLASS_NAMES[col]] = df[col]/df[0]\n",
    "        ratios.to_excel(writer, index=False, sheet_name=sub)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ddfa147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_dec(val, d=2):\n",
    "    return round(val, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7df9a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_feat_dfs = {}\n",
    "sheet_name = \"feature_importances\"\n",
    "for subreddit in SUBREDDIT_LABELS:\n",
    "    eval_filepath = f\"/home/cara/Documents/reddit_analyses/thread-size/Outputs/2_thread_size/{subreddit}/2_tuning/tuning_outputs.xlsx\"\n",
    "    df = pd.read_excel(eval_filepath, sheet_name=sheet_name)\n",
    "    \n",
    "    # Ensure correct column names\n",
    "    df = df[['feature', 'mean_importance', 'mean_split', 'mean_gain']].rename(columns={\n",
    "        'feature': 'Feature',\n",
    "        'mean_importance': 'Scaled',\n",
    "        'mean_split': 'Split',\n",
    "        'mean_gain': 'Gain'\n",
    "    })\n",
    "    df['Feature'] = df['Feature'].apply(format_label)\n",
    "    df['Scaled'] = df[\"Scaled\"].apply(round_dec, d=4)\n",
    "    df['Split'] = df[\"Split\"].apply(round_dec, d=0)\n",
    "    df['Gain'] = df[\"Gain\"].apply(round_dec, d=0)\n",
    "\n",
    "    s2_feat_dfs[subreddit] = df\n",
    "\n",
    "outfile = f\"/home/cara/Documents/reddit_analyses/thread-size/Publication_Outputs/2_Thread_Size/tuning_outputs/feature_importances.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(outfile) as writer:\n",
    "    for sub, df in s2_feat_dfs.items():\n",
    "        df.to_excel(writer, sheet_name=sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e896913",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_digits = {'colsample_bytree': 3, 'learning_rate': 3, 'max_depth':0, 'min_child_samples':0,\n",
    "       'num_leaves':0, 'reg_alpha':3, 'reg_lambda':3, 'subsample':3}\n",
    "int_cols = ['Features', 'max_depth', 'min_child_samples', 'num_leaves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "506329da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_dfs = {}\n",
    "sheet_name = \"hyperparams\"\n",
    "for subreddit in SUBREDDIT_LABELS:\n",
    "    eval_filepath = f\"/home/cara/Documents/reddit_analyses/thread-size/Outputs/2_thread_size/{subreddit}/4_model/evaluation.xlsx\"\n",
    "    df = pd.read_excel(eval_filepath, sheet_name=sheet_name)\n",
    "    # Ensure correct column names\n",
    "    df.columns = [\"Parameter\", \"Value\", \"Features\"]\n",
    "    # Forward-fill N_feats to propagate block identifiers\n",
    "    df[\"Features\"] = df[\"Features\"].ffill().astype(int)\n",
    "\n",
    "    # Pivot into the desired wide format\n",
    "    table = df.pivot(index=\"Features\", columns=\"Parameter\", values=\"Value\")\n",
    "    for col in table.columns:\n",
    "        table[col] = table[col].apply(round_dec, d=col_digits[col])\n",
    "    for col in [x for x in table.columns if x in int_cols]:\n",
    "        table[col] = table[col].astype(int)\n",
    "    hyperparam_dfs[subreddit] = table.copy()\n",
    "\n",
    "    table = table.reset_index()  # if N_feats was the index\n",
    "    \n",
    "\n",
    "    latex_str = table.to_latex(\n",
    "        index=False,                    # don’t print the row index\n",
    "        header=True,\n",
    "        float_format=\"%.3f\",\n",
    "        column_format=\"|l|r|r|r|r|r|r|r|r|\",      # LaTeX alignment (1 left + 8 right)\n",
    "        #caption=f\"{SUBREDDIT_LABELS[subreddit]} tuned thread start LightGBM hyperparameters by number of features.\",\n",
    "        #label=f\"tab:s1-{subreddit}-hyperparams\",\n",
    "        escape=True                    # so underscores in col names are not escaped weirdly\n",
    "    )\n",
    "    \n",
    "    caption = f\"Optimal LightGBM tree hyperparameters selected via cross-validated Optuna/TPE search for each number of features for {SUBREDDIT_LABELS[subreddit]}. Values represent cross-fold aggregated hyperparameters, using the mode for integer parameters and the mean for continuous parameters. These configurations were used for the final thread size model evaluation.\"\n",
    "    title = f\"{SUBREDDIT_LABELS[subreddit]} tuned thread size LightGBM hyperparameters by number of features.\"\n",
    "    full_latex = built_supp_mat_table(title, latex_str, caption)\n",
    "    # Save to file\n",
    "    with open(f\"/home/cara/Documents/reddit_analyses/thread-size/Publication_Outputs/2_Thread_Size/tuning_outputs/{subreddit}_hparams.txt\", \"w\") as f:\n",
    "        f.write(full_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1952eec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2stagemodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
